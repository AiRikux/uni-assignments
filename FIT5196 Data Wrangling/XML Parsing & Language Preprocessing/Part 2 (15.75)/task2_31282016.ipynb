{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5196 Task 2 in Assessment 1\n",
    "#### Student Name: Nabilah Anuwar\n",
    "#### Student ID: 31282016\n",
    "\n",
    "Date: 20/09/2020\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include the main libraries you used in your assignment here, e.g.,:\n",
    "* nltk (for making bigrams, unigrams, tokenization, and Porter Stemmer)\n",
    "* langid (for language detection)\n",
    "* xlrd  (for importing excel)\n",
    "* sklearn (for countvetorizer)\n",
    "* warnings (for ignoring FutureWarnings since sklearn keep putting it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.  Import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import langid\n",
    "import nltk\n",
    "import xlrd\n",
    "from nltk.stem.porter import *\n",
    "from nltk.collocations import *\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# import warnings filter\n",
    "from warnings import simplefilter\n",
    "# ignore all future warnings\n",
    "simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Coding\n",
    "\n",
    "### Set Variables\n",
    "\n",
    "First we will set variables for use later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make variables for later\n",
    "ps = PorterStemmer()\n",
    "pattern = \"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "\n",
    "# get stopwords as a list\n",
    "with open(\"part2/stopwords_en.txt\") as f:\n",
    "\tcontent = f.readlines()\n",
    "\tstopwords = [x.strip() for x in content]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Then we will set the function for opening the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_file(file):\n",
    "\ttry:\n",
    "\t\t# set up sheets dictionary to store values per sheet\n",
    "\t\tdata = {}\n",
    "\t\tsheets = {}\n",
    "\t\tids = []\n",
    "\t\ttexts = []\n",
    "\t\t# open workbook using xlrd\n",
    "\t\tworkbook = xlrd.open_workbook(file)\n",
    "\t\t# get sheet as an object\n",
    "\t\tfor sheet in workbook.sheets():\n",
    "\t\t\tsheets[sheet.name] = {\"sheet\": workbook.sheet_by_name(sheet.name), \"rows\": []}\n",
    "\t\t# go through sheets dictionary\n",
    "\t\tfor name in sheets.keys():\n",
    "\t\t\tsheet = sheets[name][\"sheet\"]\n",
    "\t\t\t# get row range in sheets for it to loop through to get values in the row\n",
    "\t\t\tfor row in range(sheet.nrows):\n",
    "\t\t\t\tsheets[name][\"rows\"].append(sheet.row(row))\n",
    "\t\t\t# loop through values in the row to differentiate columns\n",
    "\t\t\tfor column in sheets[name][\"rows\"]:\n",
    "\t\t\t\t# get index to get info later\n",
    "\t\t\t\tindex = sheets[name][\"rows\"].index(column)\n",
    "\t\t\t\tcolumn_matched = False\n",
    "\t\t\t\tfor item in column:\n",
    "\t\t\t\t\t# get item index\n",
    "\t\t\t\t\tcol_index = column.index(item)\n",
    "\t\t\t\t\t# make sure its not empty cell\n",
    "\t\t\t\t\t# wont continue to loop when text is found\n",
    "\t\t\t\t\tif item.value != \"\" and not column_matched:\n",
    "\t\t\t\t\t\tcolumn_matched = True\n",
    "\t\t\t\t\t\ttext = item.value\n",
    "\t\t\t\t\t\t# use index to get id and time\n",
    "\t\t\t\t\t\tidc = column[col_index + 1].value\n",
    "\t\t\t\t\t\ttimestamp = column[col_index + 2].value\n",
    "\t\t\t\t\t\tif text in texts or idc in ids:\n",
    "\t\t\t\t\t\t\tsheets[name][\"rows\"][index] = ['empty']\n",
    "\t\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t\telse:\n",
    "\t\t\t\t\t\t\tpass\n",
    "\t\t\t\t\t\t# append to compare if id or text is duplicate\n",
    "\t\t\t\t\t\t# if duplicate will not be assigned\n",
    "\t\t\t\t\t\tids.append(idc)\n",
    "\t\t\t\t\t\ttexts.append(text)\n",
    "\t\t\t\t\t\tsheets[name][\"rows\"][index] = {\"text\": text, \"id\": idc, \"timestamp\": timestamp}\n",
    "\t\t\t# append to finalize output file\n",
    "\t\t\tdata[name] = sheets[name][\"rows\"]\n",
    "\t\treturn data\n",
    "\texcept Exception as e:\n",
    "\t\treturn e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we only need to only get tweets that are english we will sort them with langid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lang_check(data):\n",
    "\ttry:\n",
    "\t\t# for output later\n",
    "\t\tnew_data = {}\n",
    "\t\t# make sure its a dictionary since this code is design for it\n",
    "\t\tif type(data) is not dict:\n",
    "\t\t\treturn \"The array you provided is not a dictionary\"\n",
    "\t\t# get keys in data file\n",
    "\t\tfor key in data.keys():\n",
    "\t\t\tnew_data[key] = []\n",
    "\t\t\trows = data[key]\n",
    "\t\t\tindex = 0\n",
    "\t\t\t# iterate through rows per date\n",
    "\t\t\tfor row in rows:\n",
    "\t\t\t\t# remove empty row\n",
    "\t\t\t\tif type(row) is not dict:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t# remove titles\n",
    "\t\t\t\tif row['text'] == 'text' and row['id'] == 'id' and row['timestamp'] == 'created_at':\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\ttext = str(row['text'])\n",
    "\t\t\t\tlang_test = langid.classify(text)\n",
    "\t\t\t\t# check if its english\n",
    "\t\t\t\tif lang_test[0] != 'en':\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\t\t# add row to dictionary\n",
    "\t\t\t\tnew_data[key].append(row)\n",
    "\t\t\t\tindex += 1\n",
    "\t\treturn new_data\n",
    "\texcept Exception as e:\n",
    "\t\treturn e"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer Task 2.1 we create these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams but with PorterStemmer and stopwords\n",
    "def bigrams_vocab(data):\n",
    "\t# for output later\n",
    "\t# to get all the table tokens\n",
    "\ttable_tokens = []\n",
    "\t# iterate through data keys\n",
    "\tfor key in data.keys():\n",
    "\t\trows = data[key]\n",
    "\t\t# iterate through each tweets\n",
    "\t\tfor row in rows:\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttokenizer = nltk.RegexpTokenizer(pattern)\n",
    "\t\t\t\ttokens = tokenizer.tokenize(row[\"text\"])\n",
    "\t\t\t\tnew_tokens = []\n",
    "\t\t\t\tfor word in tokens:\n",
    "\t\t\t\t\tword = word.lower()\n",
    "\t\t\t\t\t# the bigrams doesnt seem to have be a stem word so we don't use it\n",
    "\t\t\t\t\t# i got the wrong answer when i use stopwords\n",
    "\t\t\t\t\t# make sure after transform is still in limit\n",
    "\t\t\t\t\tif len(word) < 3:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnew_tokens.append(word)\n",
    "\t\t\t\ttable_tokens = table_tokens + new_tokens\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\tfinder = BigramCollocationFinder.from_words(table_tokens)\n",
    "\ta = finder.nbest(bigram_measures.pmi, 200)\n",
    "\treturn a\n",
    "\n",
    "\n",
    "def update_stopwords(data):\n",
    "\tto_stopword = []\n",
    "\tfor key in data.keys():\n",
    "\t\trows = data[key]\n",
    "\t\tsheet_tokens = []\n",
    "\t\tfor row in rows:\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttokenizer = nltk.RegexpTokenizer(pattern)\n",
    "\t\t\t\ttokens = tokenizer.tokenize(row[\"text\"])\n",
    "\t\t\t\tnew_tokens = []\n",
    "\t\t\t\tfor word in tokens:\n",
    "\t\t\t\t\tword = word.lower()\n",
    "\t\t\t\t\tif word in stopwords:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\t# stop words will not be stemmed\n",
    "\t\t\t\t\t# make sure transformed word doesn't decrease len later\n",
    "\t\t\t\t\telif len(word) < 3:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnew_tokens.append(word)\n",
    "\t\t\t\ttokens_l = [x.lower() for x in new_tokens]\n",
    "\t\t\t\tsheet_tokens = sheet_tokens + tokens_l\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\t# make sure there are no repeating values\n",
    "\t\ta = set(sheet_tokens)\n",
    "\t\tto_stopword = to_stopword + list(a)\n",
    "\t# get frequency and add to stopwords those that appear less than 5 days\n",
    "\tto_stopword = nltk.FreqDist(to_stopword)\n",
    "\tfor key in to_stopword.keys():\n",
    "\t\tif to_stopword[key] < 5:\n",
    "\t\t\tstopwords.append(key)\n",
    "\n",
    "\n",
    "def get_vocab(data):\n",
    "\tall_tokens = []\n",
    "\t# to append to list later\n",
    "\tbigrams = []\n",
    "\t# make sure its not repeating in single words\n",
    "\tbigram = []\n",
    "\tbgr = bigrams_vocab(data)\n",
    "\tfor bg in bgr:\n",
    "\t\t# make text for vocab file\n",
    "\t\ttext = bg[0]+\"_\"+bg[1]\n",
    "\t\tbigrams.append(text)\n",
    "\t\tbigram.append(bg[0])\n",
    "\t\tbigram.append(bg[1])\n",
    "\tupdate_stopwords(data)\n",
    "\t# get single vocabs\n",
    "\tfor key in data.keys():\n",
    "\t\trows = data[key]\n",
    "\t\tsheet_tokens = []\n",
    "\t\tfor row in rows:\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttokenizer = nltk.RegexpTokenizer(pattern)\n",
    "\t\t\t\ttokens = tokenizer.tokenize(row[\"text\"])\n",
    "\t\t\t\tnew_tokens = []\n",
    "\t\t\t\tfor word in tokens:\n",
    "\t\t\t\t\tword = word.lower()\n",
    "\t\t\t\t\tif word in stopwords:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telif word in bigram:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tword = ps.stem(word)\n",
    "\t\t\t\t\t# make sure after transform is still in limit\n",
    "\t\t\t\t\t# language id is not used since already since the start\n",
    "\t\t\t\t\tif len(word) < 3:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnew_tokens.append(word)\n",
    "\t\t\t\ttokens_l = [x.lower() for x in new_tokens]\n",
    "\t\t\t\tsheet_tokens = sheet_tokens + tokens_l\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\tall_tokens = all_tokens + sheet_tokens\n",
    "\tall_tokens = all_tokens + bigrams\n",
    "\t# make sure all vocabs doesnt repeat\n",
    "\tall_tokens = set(all_tokens)\n",
    "\tvocab = list(all_tokens)\n",
    "\tvocab = sorted(vocab)\n",
    "\treturn vocab\n",
    "\n",
    "\n",
    "# make file for vocab\n",
    "def make_vocab(data):\n",
    "\tvocab = get_vocab(data)\n",
    "\tf = open(\"31282016_vocab.txt\", \"w\")\n",
    "\tn = 0\n",
    "\tfor word in vocab:\n",
    "\t\ttext = str(word) + \":\" + str(n)\n",
    "\t\tf.write(text)\n",
    "\t\tf.write('\\n')\n",
    "\t\tn += 1\n",
    "\tf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer Task 2.2 we create these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make unigram\n",
    "def uni_data(data):\n",
    "\tuni_arr = {}\n",
    "\tfor key in data.keys():\n",
    "\t\trows = data[key]\n",
    "\t\tsheet_tokens = []\n",
    "\t\tfor row in rows:\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttokenizer = nltk.RegexpTokenizer(pattern)\n",
    "\t\t\t\ttokens = tokenizer.tokenize(row[\"text\"])\n",
    "\t\t\t\tnew_tokens = []\n",
    "\t\t\t\tfor word in tokens:\n",
    "\t\t\t\t\tword = word.lower()\n",
    "\t\t\t\t\tif not word.isalnum():\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telif word in stopwords:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tword = ps.stem(word)\n",
    "\t\t\t\t\t# make sure transformed word doesn't decrease len later\n",
    "\t\t\t\t\tif len(word) < 3:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnew_tokens.append(word)\n",
    "\t\t\t\ttokens_l = [x.lower() for x in new_tokens]\n",
    "\t\t\t\tsheet_tokens = sheet_tokens + tokens_l\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\t# get frequency for unigram\n",
    "\t\tfreq = nltk.FreqDist(sheet_tokens)\n",
    "\t\tuni_arr[key] = freq\n",
    "\treturn uni_arr\n",
    "\n",
    "\n",
    "# make bigram\n",
    "def bi_data(data):\n",
    "\t# create dictionary to use later\n",
    "\tbi_arr = {}\n",
    "\t# get data keys\n",
    "\tfor key in data.keys():\n",
    "\t\trows = data[key]\n",
    "\t\tsheet_tokens = []\n",
    "\t\tfor row in rows:\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# tokenize the relevant words\n",
    "\t\t\t\ttokenizer = nltk.RegexpTokenizer(pattern)\n",
    "\t\t\t\ttokens = tokenizer.tokenize(row[\"text\"])\n",
    "\t\t\t\tnew_tokens = [word.lower() for word in tokens if word.isalnum()]\n",
    "\t\t\t\t# use function to make bigrams\n",
    "\t\t\t\tbigram = nltk.bigrams(new_tokens)\n",
    "\t\t\t\ttokens_l = [x for x in bigram]\n",
    "\t\t\t\tsheet_tokens = sheet_tokens + tokens_l\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\t\t# get frequency for bigram\n",
    "\t\tfreq = nltk.FreqDist(sheet_tokens)\n",
    "\t\tbi_arr[key] = freq\n",
    "\treturn bi_arr\n",
    "\n",
    "\n",
    "# make file for unigram\n",
    "def make_uni(data):\n",
    "\t# get data file\n",
    "\tunigram = uni_data(data)\n",
    "\t# make a file\n",
    "\tf = open(\"31282016_100uni.txt\", \"w\")\n",
    "\t# iterate through keys\n",
    "\tfor key in data.keys():\n",
    "\t\t# get top 100\n",
    "\t\tuni_100 = unigram[key].most_common(100)\n",
    "\t\t# make the line needed\n",
    "\t\tline = key + \":\" + str(uni_100)\n",
    "\t\t# write line in file and add break for new line\n",
    "\t\tf.write(line)\n",
    "\t\tf.write('\\n')\n",
    "\t# close file\n",
    "\tf.close()\n",
    "\n",
    "\n",
    "# make file for bigrams\n",
    "def make_bi(data):\n",
    "\t# get data file\n",
    "\tbigram = bi_data(data)\n",
    "\t# make file\n",
    "\tf = open(\"31282016_100bi.txt\", \"w\")\n",
    "\t# iterate through keys\n",
    "\tfor key in data.keys():\n",
    "\t\t# get top 100\n",
    "\t\tbi_100 = bigram[key].most_common(100)\n",
    "\t\t# write line in file and break for new line\n",
    "\t\tline = key + \":\" + str(bi_100)\n",
    "\t\tf.write(line)\n",
    "\t\tf.write('\\n')\n",
    "\t# close file\n",
    "\tf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Task 2.3 we created these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def countvec(data):\n",
    "\t# update stopwords\n",
    "\t# update_stopwords(data)\n",
    "\t# create dictionary to use later\n",
    "\tfreqy = {}\n",
    "\trow_l = {}\n",
    "\tcv_l = []\n",
    "\tfor key in data.keys():\n",
    "\t\trows = data[key]\n",
    "\t\tsheet_tokens = []\n",
    "\t\t# tokenize at first\n",
    "\t\tfor row in rows:\n",
    "\t\t\ttry:\n",
    "\t\t\t\ttokenizer = nltk.RegexpTokenizer(pattern)\n",
    "\t\t\t\ttokens = tokenizer.tokenize(row[\"text\"])\n",
    "\t\t\t\tnew_tokens = []\n",
    "\t\t\t\tfor word in tokens:\n",
    "\t\t\t\t\tword = word.lower()\n",
    "\t\t\t\t\tif not word.isalnum():\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telif word in stopwords:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tword = ps.stem(word)\n",
    "\t\t\t\t\t# make sure transformed word doesn't decrease len later\n",
    "\t\t\t\t\tif len(word) < 3:\n",
    "\t\t\t\t\t\tcontinue\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tnew_tokens.append(word)\n",
    "\t\t\t\ttokens_l = [x.lower() for x in new_tokens]\n",
    "\t\t\t\tsheet_tokens = sheet_tokens + tokens_l\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tpass\n",
    "\t\t# get tokenize  words\n",
    "\t\trow_l[key] = sheet_tokens\n",
    "\t\t# join all the keys to be used in countvectorization\n",
    "\t\trow = \" \".join(row_l[key])\n",
    "\t\tcv_l.append(row)\n",
    "\t\t# get count from tokenize works\n",
    "\t\tfreq = nltk.FreqDist(sheet_tokens)\n",
    "\t\tfreqy[key] = dict(freq)\n",
    "\t# we can set min_df to ignore words that appear in less than 5 days\n",
    "\t# but we have the stopwords list already for that\n",
    "\tcv = CountVectorizer()\n",
    "\tcv.fit_transform(cv_l)\n",
    "\t# get index\n",
    "\tdicts = cv.vocabulary_\n",
    "\tfor key in data.keys():\n",
    "\t\trows = row_l[key]\n",
    "\t\tcount = freqy[key]\n",
    "\t\t# set list for words to append in result\n",
    "\t\twords = []\n",
    "\t\tfor row in rows:\n",
    "\t\t\t# get values from the dictionaries using the word as key\n",
    "\t\t\tindex = dicts[row]\n",
    "\t\t\tcounty = count[row]\n",
    "\t\t\ttext = \"{}:{}\".format(index, county)\n",
    "\t\t\twords.append(text)\n",
    "\t\t\tcontinue\n",
    "\t\trow_l[key] = words\n",
    "\treturn row_l\n",
    "\n",
    "\n",
    "def make_countvec(data):\n",
    "\tday = countvec(data)\n",
    "\tf = open(\"31282016_countVec.txt\", \"w\")\n",
    "\tfor key in day.keys():\n",
    "\t\t# write in date\n",
    "\t\tf.write(key)\n",
    "\t\twords = day[key]\n",
    "\t\t# get the values in dictionary\n",
    "\t\tfor word in words:\n",
    "\t\t\t# write accordingly\n",
    "\t\t\tf.write(\",\")\n",
    "\t\t\tf.write(word)\n",
    "\t\tf.write('\\n')\n",
    "\tf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open_file(\"part2/31282016.xlsx\")\n",
    "print(\"finished open file\")\n",
    "data = lang_check(data)\n",
    "print(\"finished language check\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_vocab(data)\n",
    "print(\"vocabulary list is finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_bi(data)\n",
    "make_uni(data)\n",
    "print(\"bigrams and unigrams are finished\")\n",
    "make_countvec(data)\n",
    "print(\"finished all task, please check output files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Summary\n",
    "Give a short summary of your work done above, such as your findings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* It was interesting because I was closer to the answer when bigrams aren't sorted through the stopwords yet unigrams have to be sorted through. \n",
    "* the regex also sorted through random words such as link but we use langid to tackle that\n",
    "* sklearn kept having FutureWarnings that i have to ignore it as the code continued even if it appeared\n",
    "* vocabulary maker is the function that took the longest out of them all as it has to iterate through all the available words"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
